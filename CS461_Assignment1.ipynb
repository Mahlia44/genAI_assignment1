{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#FFFFFF\">   \n",
    "  <tr>     \n",
    "  <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Logo_EPFL_2019.svg\" width=\"150x\"/>\n",
    "  </td>     \n",
    "  <td>\n",
    "  <h1> <b>CS-461: Foundation Models and Generative AI</b> </h1>\n",
    "  Prof. Charlotte Bunne  \n",
    "  </td>   \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Graded Assignment 1  \n",
    "### CS-461: Foundation Models and Generative AI - Fall 2025  - Due: October 8, 23:59 CET\n",
    "\n",
    "Welcome to the first graded assignment!\n",
    "In this assignment, you will **implement and explore self-supervised learning** on a downsampled subset of the [ImageNet-1k dataset](https://www.image-net.org/), and evaluate how well your model generalizes **both in-distribution and out-of-distribution (OOD)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By completing this assignment, you will learn to:\n",
    "- Implement a custom **encoder** and **projection head** for images  \n",
    "- Experiment with **data augmentations** for self-supervised learning  \n",
    "- Train a model using a **self-supervised loss**  \n",
    "- Evaluate learned representations with **k-NN** and **linear probes**  \n",
    "- Assess **out-of-distribution (OOD) generalization** to unseen classes  \n",
    "- Save, visualize, and submit results in a reproducible way  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Practical Notes\n",
    "- **Dataset:**  \n",
    "  - Training: 200 ImageNet classes, 500 images each (100k total)  \n",
    "  - Validation: 200 ImageNet classes, 50 images each (10k total)  \n",
    "  - **OOD dataset:** 200 unseen classes, 50 images each (10k total)  \n",
    "- Use OOD only for **evaluation**, never for training.  \n",
    "- Checkpoints and evaluation intervals are already set up ‚Äî your main tasks are to fill in missing functions and customize the model.  \n",
    "- Some helper utilities (e.g., dataset loaders, probes) are provided in `utils.py`.  \n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Deliverables:** You will submit:\n",
    "- Your modified **`models.py`**  \n",
    "- Trained weights in **`final_model.safetensors`**  \n",
    "- A short **report.md** (max 500 words) ‚Äî including **discussion of OOD results**  \n",
    "- This completed notebook **CS461_Assignment1.ipynb**  \n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Important:** Don‚Äôt forget to fill in your **SCIPER number** and **full name** in Section 0, otherwise you will receive **0 points**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import packages and set up the device. \\\n",
    "Feel free to add any additional packages you may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads modules when you make changes (useful during development)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üÜî 0. SCIPER Number and Name  \n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT!** ‚ö†Ô∏è  \n",
    "You **must** fill in your **SCIPER number** and **full name** below.  \n",
    "\n",
    "This is **required for automatic grading**.  \n",
    "If you do **not** provide this information, you will receive **0Ô∏è‚É£ (zero)** for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIPER = 345625  \n",
    "LAST_NAME = \"Merville-Hipeau\"\n",
    "FIRST_NAME = \"Mahlia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets & Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we will work with a subset of the ImageNet-1k dataset: color images downsampled to 64√ó64, covering 200 classes.\n",
    "- The training set contains 500 images per class (100,000 images in total), and the validation set contains 50 images per class (10,000 images in total).\n",
    "- The Out-Of-Distribution (OOD) datasets contain images from classes not present in the training set. It contains 50 images from 200 different classes (1,000 images in total).\n",
    "- The purpose of these OOD datasets is to evaluate the generalization capabilities of the learned representations. You should not use it for training.\n",
    "- During evalution, we will measure your model's performance on another OOD dataset (different from the one provided here), so make sure to not overfit on the provided OOD dataset.\n",
    "\n",
    "<!-- Let's download/load it and define a default transformation turning a PIL Image into a `torch.tensor` -->\n",
    "Make sure that you have access to the `/shared/CS461/cs461_assignment1_data/` folder. The folder structure should look like this:\n",
    "```\n",
    "cs461_assignment1_data/\n",
    "‚îî‚îÄ‚îÄ train.npz\n",
    "‚îî‚îÄ‚îÄ val.npz\n",
    "‚îî‚îÄ‚îÄ ood.npz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset class and other utilities you developed in previous homeworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearnex not installed, using standard sklearn\n"
     ]
    }
   ],
   "source": [
    "from utils import ImageDatasetNPZ, default_transform, seed_all\n",
    "from utils import run_knn_probe, run_linear_probe, extract_features_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, you can use the provided `seed_all` function to set the random seed for all relevant libraries (Python, NumPy, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)  # For reproducibility, you can use any integer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably want to implement custom data augmentations for the self-supervised learning method you choose. \\\n",
    "Feel free to swap the `default_transform` defined below and create multiple instances of datasets with different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path('/shared/CS461/cs461_assignment1_data/')\n",
    "data_dir = Path('cs461_assignment1_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split the provided OOD dataset into a training and validation set using the code below. \\\n",
    "You should not use the training split for actually training your models, but only for evaluation (e.g. kNN or linear probing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ds_ood = ImageDatasetNPZ(data_dir / 'ood.npz', transform=default_transform)\n",
    "ood_val_ratio = 0.2\n",
    "train_mask = rng.permutation(len(ds_ood)) >= int(len(ds_ood) * ood_val_ratio)\n",
    "ds_oods_train = torch.utils.data.Subset(ds_ood, np.where(train_mask)[0])\n",
    "ds_oods_val = torch.utils.data.Subset(ds_ood, np.where(~train_mask)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # 1500\n",
    "num_workers = 4\n",
    "pin_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRTransform:\n",
    "\n",
    "    def __init__(self, size=32, s=0.5, blur_p=0.5):\n",
    "        color_jitter = T.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "        k = 3 if size <= 32 else 5\n",
    "        base = [\n",
    "            T.RandomResizedCrop(size=size, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply([color_jitter], p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=k, sigma=(0.1, 2.0))], p=blur_p),\n",
    "            T.ToTensor()\n",
    "        ]\n",
    "        self.train_transform = T.Compose(base)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs1, xs2, ys = [], [], []\n",
    "    for (x1, x2), y in batch:\n",
    "        xs1.append(x1)\n",
    "        xs2.append(x2)\n",
    "        ys.append(y)\n",
    "    return torch.stack(xs1), torch.stack(xs2), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_transform = SimCLRTransform(size=32)\n",
    "\n",
    "train_dataset = ImageDatasetNPZ(data_dir / 'train.npz', transform=simclr_transform)\n",
    "val_dataset = ImageDatasetNPZ(data_dir / 'val.npz', transform=simclr_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader  = DataLoader(val_dataset,  batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Your Model\n",
    "\n",
    "- Load your model from `models.py`.\n",
    "- You will need to modify the `encoder` and `projection` modules, as the provided template implementation is only a placeholder.\n",
    "- You SHOULD NOT change the `input_dim`, `input_channels`, and `feature_dim` parameters of the `ImageEncoder` class.\n",
    "- You can use an existing architecture (e.g., ResNet, ViT) but you SHOULD NOT use any pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (encoder): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=1000, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import ImageEncoder\n",
    "\n",
    "model = ImageEncoder().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helpers for Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest you to implement the following helper functions to keep your training and evaluation loops clean and organized. \\\n",
    "- `training_step`: Performs a single training step (forward pass, loss computation, backward pass, optimizer step) and returns the loss value.\n",
    "- `evaluation_step`: Evaluates the model on the validation dataset and returns the accuracy.\n",
    "\n",
    "Depending on your specific requirements, you may also want to implement additional utility functions for tasks such as data loading, metric computation, and logging.\n",
    "\n",
    "As you have seen from previous assignments, loss functions for self-supervised learning objectives can be quite complex. \\\n",
    "Feel free to implement any helper functions you may need to compute the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent(z1, z2, tau=0.5):\n",
    "    \"\"\"\n",
    "    Computes NT-Xent loss.\n",
    "    z1: (batch_size, feature_dim) tensor of normalized projection vectors\n",
    "    z2: (batch_size, feature_dim) tensor of normalized projection vectors\n",
    "    returns: loss (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    B, d = z1.shape\n",
    "    z = torch.cat([z1, z2], dim=0)              # (2B, d)\n",
    "    sim = (z @ z.t()) / tau                     # (2B, 2B)\n",
    "    mask = torch.eye(2*B, dtype=torch.bool, device=z.device)\n",
    "    sim.masked_fill_(mask, -1e9)\n",
    "    targets = torch.arange(B, device=z.device)\n",
    "    targets = torch.cat([targets + B, targets], dim=0)\n",
    "    \n",
    "    return F.cross_entropy(sim, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, x1, x2, y_true, loss_fn, optimizer, device): # *args, **kwargs\n",
    "    \n",
    "    model.train()\n",
    "    x1, x2, y_true = x1.to(device), x2.to(device), y_true.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    z1 = model(x1)\n",
    "    z2 = model(x2)\n",
    "\n",
    "    # Loss computation\n",
    "    loss = nt_xent(z1, z2)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()         \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN evaluation\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluation_step(model, train_loader, val_loader, k, device=\"cuda\"):\n",
    "    model.eval()\n",
    "\n",
    "    train_feats, train_labels = extract_features_and_labels(model, train_loader, normalize=True)\n",
    "    val_feats, val_labels = extract_features_and_labels(model, val_loader, normalize=True)\n",
    "\n",
    "    # Fit k-NN on train, evaluate on val\n",
    "    acc = run_knn_probe(train_feats, train_labels, val_feats, val_labels)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to adapt and add more arguments\n",
    "lr = 1e-3\n",
    "weight_decay = 5e-2\n",
    "lr_step_size = 10\n",
    "lr_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt your training configuration and implement the training loop. \\\n",
    "You probably want to save model checkpoints and evaluate the model on the validation set at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50  # Adjust the number of epochs as needed\n",
    "eval_interval = 5  # Evaluate the model every 'eval_interval' epochs\n",
    "save_interval = 10  # Save the model every 'save_interval' epochs\n",
    "\n",
    "checkpoints_dir = Path('checkpoints')\n",
    "if not checkpoints_dir.exists():\n",
    "    checkpoints_dir.mkdir(parents=True, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|‚ñä         | 4/50 [04:58<57:01, 74.38s/it]  "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Training Progress\"):\n",
    "    #  TRAINING\n",
    "    running_loss = 0.0\n",
    "    for x1, x2, y_true in train_loader:\n",
    "        loss = training_step(model, x1, x2, y_true, nt_xent, optimizer, device)\n",
    "        running_loss += loss\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    #  LEARNING RATE UPDATE\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    #  EVALUATION (every few epochs)\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        val_acc = evaluation_step(model, train_loader, val_loader, k=200, device=device)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] | \"\n",
    "              f\"Train Loss: {avg_loss:.4f} | \"\n",
    "              f\"Val k-NN-200 Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    #  SAVE CHECKPOINTS\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        ckpt_path = os.path.join(checkpoints_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "        save_model(model, ckpt_path)\n",
    "        print(f\"Model checkpoint saved at {ckpt_path}\")\n",
    "\n",
    "#  SAVE FINAL MODEL\n",
    "final_model_path = os.path.join(checkpoints_dir, \"model_final.pt\")\n",
    "save_model(model, final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the performance of your trained model, visualize some results. \\\n",
    "You can visualize:\n",
    "- Sample images from the validation set along with their predicted labels.\n",
    "- Training and validation loss curves over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loss and accuracy curves\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_accuracies, label=\"Validation k-NN Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Validation Accuracy Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Get a small batch from validation loader\n",
    "images, labels = next(iter(val_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Move tensors to CPU for visualization\n",
    "images = images.cpu()\n",
    "labels = labels.cpu()\n",
    "preds = preds.cpu()\n",
    "\n",
    "# Plot a few examples\n",
    "num_show = 6\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_show):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    img = images[i].permute(1, 2, 0)  # CxHxW -> HxWxC\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Pred: {preds[i].item()} | True: {labels[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must submit the following files:\n",
    "- `models.py`: Contains the implementation of your model architecture.\n",
    "- `final_model.safetensors`: The trained model weights saved in the safetensors format.\n",
    "- `report.md`: A brief report summarizing your approach, design choices, and results.\n",
    "- `CS461_Assignment1.ipynb`: The Jupyter notebook containing your code and explanations. Make sure to save your progress before running the cell below.\n",
    "\n",
    "You will submit your assignment under a single folder named `/home/cs461_assignment1_submission` containing the above files. \\\n",
    "Make sure to replace `<SCIPER>`, `<LAST_NAME>`, and `<FIRST_NAME>` with your actual SCIPER number, last name, and first name respectively. \\\n",
    "The following cell will help you move the files into the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path('.')\n",
    "output_dir = Path.home() / 'cs461_assignment1_submission'\n",
    "\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "shutil.copy(final_model_path, output_dir / 'final_model.safetensors')\n",
    "shutil.copy(work_dir / 'models.py', output_dir / 'models.py')\n",
    "shutil.copy(work_dir / 'CS461_Assignment1.ipynb', output_dir / 'CS461_Assignment1.ipynb')\n",
    "shutil.copy(work_dir / 'report.md', output_dir / 'report.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all required files are present in the submission folder before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert SCIPER is not None and LAST_NAME is not None and FIRST_NAME is not None, \"Please set your SCIPER, LAST_NAME, and FIRST_NAME variables.\"\n",
    "\n",
    "list_of_files = ['final_model.safetensors', 'models.py', 'CS461_Assignment1.ipynb', 'report.md']\n",
    "files_found = all((output_dir / f).exists() for f in list_of_files)\n",
    "assert files_found, f\"One or more required files are missing in the submission folder: {list_of_files}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test whether your submission folder is appropriately structured by using the `eval.py`:\n",
    "```bash\n",
    "python eval.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment the line below to run the evaluation script and check your model's performance\n",
    "\n",
    "# !python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "üéâ **Congratulations!**  \n",
    "You‚Äôve completed Assignment 1. Good luck, and don‚Äôt forget to double-check your submission!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
